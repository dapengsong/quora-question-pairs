{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning bert for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes fine tuning of bert model. Expected results after fine-tuning is\n",
    "* 0.9-0.95 Accuracy\n",
    "* 0.9-0.95 f1\n",
    "\n",
    "before fine-tuning both metics are around 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from transformers.transformers import AdamW, WarmupLinearSchedule\n",
    "from transformers.transformers import BertTokenizer, BertModel, BertConfig, BertForSequenceClassification\n",
    "\n",
    "from utils import cache_ds, evaluate, process_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv', index_col='id')\n",
    "train.dropna(axis=0, inplace=True)\n",
    "test = pd.read_csv('data/test.csv', index_col='test_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = 'bert-base-cased-finetuned-mrpc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_weights, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=BertForSequenceClassification.from_pretrained(model_weights, output_hidden_states=True).cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_ds(train, tokenizer, save = './data/train_ds_CASED_cached')\n",
    "#train_tensor_data = torch.load('./data/train_ds_CASED_cached')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = torch.utils.data.random_split(train_tensor_data, [len(train_tensor_data) - 10000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(train_ds, batch_size=32, sampler=train_sampler)\n",
    "dl_val = DataLoader(val_ds, batch_size=8, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "t_total = n_epochs * len(dl_train)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=0, t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_grad_norm = 1\n",
    "global_step = 0\n",
    "acc_loss = 0.0\n",
    "logging_loss = acc_loss\n",
    "model.zero_grad()\n",
    "log_step = 500\n",
    "max_steps = 15000\n",
    "\n",
    "epoch_range = trange(n_epochs, desc='Epoch', position=0, leave=True)\n",
    "\n",
    "for epoch_num in epoch_range:\n",
    "    epoch_iter = tqdm(dl_train, desc='Inside epoch {}'.format(epoch_num), position=1, leave=True)\n",
    "    for step, batch in enumerate(epoch_iter):\n",
    "        model.train()\n",
    "        \n",
    "        tup = tuple(item.cuda() for item in batch[3:])\n",
    "        model_input = dict(zip(['input_ids', 'attention_mask', 'token_type_ids', 'labels'], tup))\n",
    "        logloss, logits = model(**model_input)[:2]\n",
    "        \n",
    "        logloss.backward()\n",
    "        acc_loss += logloss.item()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "        global_step += 1\n",
    "        \n",
    "        if global_step % log_step == 0:\n",
    "            eval_res = evaluate(model)\n",
    "            for key, value in eval_res:\n",
    "                writer.add_scalar('eval_{}'.format(key), value, global_step=global_step)\n",
    "            writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "            writer.add_scalar('loss', (acc_loss - logging_loss) / log_step, global_step)\n",
    "            logging_loss = acc_loss\n",
    "    \n",
    "        if global_step >= max_steps:\n",
    "            break\n",
    "    \n",
    "    torch.save(\n",
    "        {\n",
    "            'epoch': epoch_num,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            #'loss': loss,\n",
    "            'val_metric': list(evaluate(model))\n",
    "        }, 'models/checkpoint_iter_{}_{}'.format(global_step, datetime.now()))\n",
    "    \n",
    "    if global_step >= max_steps:\n",
    "        break\n",
    "\n",
    "        \n",
    "        \n",
    "writer.export_scalars_to_json('./scalars_{}.json'.format(datetime.now()))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./models/checkpoint_iter_15000_2019-11-04 14:43:10.972407')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('logloss', tensor(2.8814e-05, device='cuda:0')),\n",
       " ('accuracy', 0.9354),\n",
       " ('f1', 0.9105263157894737)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check that everything's ok\n",
    "list(evaluate(model, dl_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_test_ds = cache_ds(test.dropna(), save='./data/test_ds_CASED_cached', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampler = torch.utils.data.SequentialSampler(test_ds)\n",
    "dl_test = DataLoader(test_ds, batch_size=100, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = process_test(model, dl_test)\n",
    "answers = np.concatenate(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['is_duplicate'] = 0\n",
    "test.loc[test.dropna().index, 'is_duplicate'] = answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_name = #INSERT SUBMISSION NAME HERE\n",
    "test[['is_duplicate']].to_csv(your_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
